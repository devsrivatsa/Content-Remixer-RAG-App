{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth \"xformers==0.0.28.post2\"\n",
    "# Also get the latest nightly Unsloth!\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install -Uq datasets bitsandbytes accelerate einops transformers peft trl sentencepiece comet-ml>=3.43.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMET_API_KEY = \"cN46R8yZC3C1aWC7tTon4TKM4\"\n",
    "HF_TOKEN = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/devsrivatsa/llm-engineers-handbook-dpo/0dd5abe8fb1948c98d3ee01b252d2fdb\n",
      "\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/workspace' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n"
     ]
    }
   ],
   "source": [
    "import comet_ml\n",
    "# comet_ml.login(COMET_API_KEY)\n",
    "exp = comet_ml.start(project_name=\"llm-engineers-handbook_dpo\", api_key=COMET_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import TextStreamer\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from datasets import load_dataset, concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2024.12.4: Fast Llama patching. Transformers:4.46.3.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A6000. Max memory: 47.529 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf374acf55f4a13a0889cae5aec7139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "090a075242114de8a639e4c8ddfac04f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd7e57a06de3446da25ae353f3e2129f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f5d29697ccd4042bea5cc8efb08b82d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2a584b50ef449a692f6926b76e2e6d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b629137387e412dad58d2994f4d196a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f9fb673b21f40aeb9b261a6042284be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3fcf42b6222496f9b923084ce70b53d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/230 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "104007b8019844929fb4d86e586c6e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5944b7e0cf94eb6ba66d06075bbc755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad17ead61d9c428fab7bd4395a4f9b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#     model_name=\"meta-llama/Meta-Llama-3.1-8b\",\n",
    "#     max_seq_length=max_seq_length,\n",
    "#     load_in_4bit=True,\n",
    "#     device_map=\"sequential\",\n",
    "#     trust_remote_code=True,\n",
    "#     use_gradient_checkpointing=\"unsloth\",\n",
    "# )\n",
    "#########################################################################\n",
    "#############finetuning from prev checkpoint#############################\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"srivatsaHFhub/llama3.1_fineTomeAlpaca_modified\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=False,\n",
    "    device_map=\"sequential\",\n",
    "    trust_remote_code=True,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.12.4 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    use_gradient_checkpointing=True,\n",
    "    use_rslora=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0284b08fd64243848528a0c21ce0324e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/452 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3148ea98fb5d4e68825451cd6d3769d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/980k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f2a09bb6ee4cb2886f2dc31a263a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/112k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a03b1852d7664706b5a3f7209e9195e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3001 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bc9f3930bc84a05b4af1b17958b782c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/334 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13736c23dbb8484c943b7297f329d6d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/408 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9124ae0bfad24791867822b9f963c202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/89.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0145587578c54c0bb03d0139dd3edccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset1 = load_dataset(\"mlabonne/llmtwin\")\n",
    "dataset2 = load_dataset(\"mlabonne/FineTome-Alpaca-100k\", split=\"train[:10000]\")\n",
    "dataset = concatenate_datasets([dataset1[\"train\"], dataset2]).remove_columns([\"source\", \"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6017</th>\n",
       "      <td>Can you explain to me the basic rules of Engli...</td>\n",
       "      <td>Sure, I will break down the basic rules of Eng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1938</th>\n",
       "      <td>Describe the five main steps of a linear optim...</td>\n",
       "      <td>The five main steps of any linear optimization...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>Describe the importance of dataset preparation...</td>\n",
       "      <td>Dataset preparation is crucial in the fine tun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5432</th>\n",
       "      <td>A student is conducting an experiment to deter...</td>\n",
       "      <td>To create a block design for this experiment, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7282</th>\n",
       "      <td>Create a function named \"is_rotation\" that tak...</td>\n",
       "      <td>To create the \"is_rotation\" function, we can c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1302</th>\n",
       "      <td>Explain the role of Comet ML in model training...</td>\n",
       "      <td>Comet ML serves as a comprehensive machine lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12907</th>\n",
       "      <td>Write Python code to solve the task:\\nGiven an...</td>\n",
       "      <td>Step 1:  To solve this problem, we need to fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8924</th>\n",
       "      <td>Explain function overloading in C++ and provid...</td>\n",
       "      <td>Function overloading in C++ allows multiple fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6738</th>\n",
       "      <td>Please provide step-by-step instructions on ho...</td>\n",
       "      <td>Sure! Here are the steps to represent a binary...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6533</th>\n",
       "      <td>Create a Java program that takes an integer as...</td>\n",
       "      <td>1. The instruction is to create a Java program...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             instruction  \\\n",
       "6017   Can you explain to me the basic rules of Engli...   \n",
       "1938   Describe the five main steps of a linear optim...   \n",
       "1063   Describe the importance of dataset preparation...   \n",
       "5432   A student is conducting an experiment to deter...   \n",
       "7282   Create a function named \"is_rotation\" that tak...   \n",
       "1302   Explain the role of Comet ML in model training...   \n",
       "12907  Write Python code to solve the task:\\nGiven an...   \n",
       "8924   Explain function overloading in C++ and provid...   \n",
       "6738   Please provide step-by-step instructions on ho...   \n",
       "6533   Create a Java program that takes an integer as...   \n",
       "\n",
       "                                                  output  \n",
       "6017   Sure, I will break down the basic rules of Eng...  \n",
       "1938   The five main steps of any linear optimization...  \n",
       "1063   Dataset preparation is crucial in the fine tun...  \n",
       "5432   To create a block design for this experiment, ...  \n",
       "7282   To create the \"is_rotation\" function, we can c...  \n",
       "1302   Comet ML serves as a comprehensive machine lea...  \n",
       "12907  Step 1:  To solve this problem, we need to fin...  \n",
       "8924   Function overloading in C++ allows multiple fu...  \n",
       "6738   Sure! Here are the steps to represent a binary...  \n",
       "6533   1. The instruction is to create a Java program...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.to_pandas().sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2209</th>\n",
       "      <td>Explain the importance of a well-defined archi...</td>\n",
       "      <td>A well-defined architecture in machine learnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2667</th>\n",
       "      <td>Discuss the significance of RAG in relation to...</td>\n",
       "      <td>RAG, or Retrieval-Augmented Generation, is par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Describe the training process of the agent.</td>\n",
       "      <td>The training process of the agent involves mul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2900</th>\n",
       "      <td>Identify and explain the components of the inf...</td>\n",
       "      <td>The inference pipeline comprises several key c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>Describe the process of extracting post texts ...</td>\n",
       "      <td>The extraction of post texts and images is acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>Discuss the importance of data quality in fine...</td>\n",
       "      <td>The quality of your dataset is crucial for the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Explain the significance of understanding the ...</td>\n",
       "      <td>A thorough understanding of the optimization p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>Outline the process of building a local LLM RA...</td>\n",
       "      <td>Building a local LLM RAG (Retrieval-Augmented ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>Explain the process of creating a default payl...</td>\n",
       "      <td>Creating a default payload within the Inferenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>Discuss the significance of data versioning in...</td>\n",
       "      <td>Data versioning plays a crucial role in MLOps ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            instruction  \\\n",
       "2209  Explain the importance of a well-defined archi...   \n",
       "2667  Discuss the significance of RAG in relation to...   \n",
       "161         Describe the training process of the agent.   \n",
       "2900  Identify and explain the components of the inf...   \n",
       "560   Describe the process of extracting post texts ...   \n",
       "2008  Discuss the importance of data quality in fine...   \n",
       "32    Explain the significance of understanding the ...   \n",
       "893   Outline the process of building a local LLM RA...   \n",
       "1075  Explain the process of creating a default payl...   \n",
       "1017  Discuss the significance of data versioning in...   \n",
       "\n",
       "                                                 output  \n",
       "2209  A well-defined architecture in machine learnin...  \n",
       "2667  RAG, or Retrieval-Augmented Generation, is par...  \n",
       "161   The training process of the agent involves mul...  \n",
       "2900  The inference pipeline comprises several key c...  \n",
       "560   The extraction of post texts and images is acc...  \n",
       "2008  The quality of your dataset is crucial for the...  \n",
       "32    A thorough understanding of the optimization p...  \n",
       "893   Building a local LLM RAG (Retrieval-Augmented ...  \n",
       "1075  Creating a default payload within the Inferenc...  \n",
       "1017  Data versioning plays a crucial role in MLOps ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1[\"train\"].to_pandas().sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e40ec06727054392b995431a16756a5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13001 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecd758737ad449ba86ca84cca06866bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13001 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10767</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4490</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5667</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5137</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8090</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8158</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12203</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "10767  <|begin_of_text|><|start_header_id|>system<|en...\n",
       "280    <|begin_of_text|><|start_header_id|>system<|en...\n",
       "4490   <|begin_of_text|><|start_header_id|>system<|en...\n",
       "5667   <|begin_of_text|><|start_header_id|>system<|en...\n",
       "5137   <|begin_of_text|><|start_header_id|>system<|en...\n",
       "2998   <|begin_of_text|><|start_header_id|>system<|en...\n",
       "8090   <|begin_of_text|><|start_header_id|>system<|en...\n",
       "8158   <|begin_of_text|><|start_header_id|>system<|en...\n",
       "1550   <|begin_of_text|><|start_header_id|>system<|en...\n",
       "12203  <|begin_of_text|><|start_header_id|>system<|en..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "def apply_basic_chat_template(example):\n",
    "    system_prompt = \"You are a helpful assistant. Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": example[\"instruction\"],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": example[\"output\"]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(apply_basic_chat_template, remove_columns=[\"instruction\", \"output\"])\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"llama-3\",\n",
    "    map_eos_token=True\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    msgs = examples[\"messages\"]\n",
    "    texts = [tokenizer.apply_chat_template(msg, tokenize=False, add_generation_prompt=False) for msg in msgs]\n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True).remove_columns([\"messages\"])\n",
    "\n",
    "dataset.to_pandas().sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 12350\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 651\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.05)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09be33c3cb9b46a3a765f590e31e4d58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8df55d9360e2486aa37145a4c8da649b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=True,\n",
    "    args=SFTConfig(\n",
    "        learning_rate=3e-4,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        per_device_train_batch_size=16,\n",
    "        gradient_accumulation_steps=2,\n",
    "        num_train_epochs=3,\n",
    "        fp16= not is_bfloat16_supported(),\n",
    "        bf16= is_bfloat16_supported(),\n",
    "        logging_steps=2,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        warmup_ratio=0.1,\n",
    "        eval_strategy=\"epoch\",\n",
    "        output_dir=\"model_output\",\n",
    "        seed=432,\n",
    "        report_to=\"comet_ml\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()\n",
    "# print(\"\\n------------------Training Complete--------------------------\\n\")\n",
    "# exp.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: You are pushing to hub, but you passed your HF username = srivatsaHFhub.\n",
      "We shall truncate srivatsaHFhub/llama3.1_fineTomeAlpaca_modified to llama3.1_fineTomeAlpaca_modified\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 353.94 out of 503.71 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 39.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3675fd8bb945fe90181a199ce7b30e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/582 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e0a086f9163438ba56a1ec5e6565d09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96c381439ad8414394c400d6a1e39884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9a3504f2f034e098157983c9891c9c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04b0b5db45914646abf5660b99a82dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bca2dac255848afac997f87d3abd66b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Saved merged model to https://huggingface.co/srivatsaHFhub/llama3.1_fineTomeAlpaca_modified\n"
     ]
    }
   ],
   "source": [
    "model.push_to_hub_merged(\"srivatsaHFhub/llama3.1_fineTomeAlpaca_modified\", tokenizer, save_method=\"merged_16bit\", token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA RTX A6000. Max memory = 47.529 GB.\n",
      "31.637 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "def gpu_status():\n",
    "    gpu_stats = torch.cuda.get_device_properties(0)\n",
    "    start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "    print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "    print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
    "\n",
    "gpu_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "def generate(prompt, max_new_tokens=512, temp=1):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\":\"You are a helpful assistant. Below is an instruction that describes a task. Write a response that appropriately completes the request.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize = True,\n",
    "        add_generation_prompt = True, # Must add for generation\n",
    "        return_tensors = \"pt\",\n",
    "        torch_dtype = torch.bfloat16\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "    model.eval()\n",
    "    res = model.generate(\n",
    "        input_ids = inputs,\n",
    "        streamer = text_streamer,\n",
    "        max_new_tokens = max_new_tokens,\n",
    "        use_cache = True,\n",
    "        temperature = temp,\n",
    "        min_p = 0.1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervised fine tuning is a technique used to enhance the performance of language models by providing them with labeled training data. This approach involves fine-tuning the model on a specific dataset, which can be tailored to improve its ability to generate contextually relevant and accurate responses. By utilizing supervision, the model can learn to better understand the nuances of language and user intent, leading to improved outputs in real-world applications. This method allows for the customization of the model to specific tasks or domains, making it an effective solution for enhancing the functionality of language models.<|reserved_special_token_158|><|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "generate(\"Can you tell me about supervised fine tuning ?\",  max_new_tokens=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct preference optimization works by using the feedback from human evaluators to directly train the model on what preferences to prioritize. This method allows for a more targeted approach to training, as it focuses on the specific aspects of the model's behavior that are deemed desirable. The process involves collecting evaluations from evaluators who rate the model's outputs based on their preferences. The model is then updated based on these evaluations, gradually learning to generate outputs that better align with the specified preferences. This approach can be particularly effective when working with limited training data or when trying to fine-tune a model for a specific application domain.<|reserved_special_token_53|><|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "generate(\"How does direct preference optimization work ?\", max_new_tokens=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature in the context of language model output is a parameter that controls the randomness of the generated text. By adjusting the temperature value, we can influence the diversity and coherence of the LLM's responses. A higher temperature value generally leads to more varied and unpredictable text, while a lower temperature value tends to produce more focused and coherent text.\n",
      "\n",
      "In summary, temperature is a valuable tool in fine-tuning LLM outputs, allowing users to tailor the generated text "
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "generate(\"How does temperature affect LLM output ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "gpu_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"srivatsaHFhub/llama3.1_fineTomeAlpaca_modified\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=False,\n",
    "    device_map=\"sequential\",\n",
    "    trust_remote_code=True,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**These generations are not very precise and to the point, and have repeted information.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import PatchDPOTrainer\n",
    "from trl import DPOConfig, DPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PatchDPOTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.12.4: Fast Llama patching. Transformers:4.46.3.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A6000. Max memory: 47.438 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d82cd267a0a47739c6969e4cb337fa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "449013f526f14567b77fff5bcd57ced2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b2370bc3fb4660ac97b2c6d713e608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1355f1ccc18947ecbd9e762e5bd1beb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b052942e2c1744f08e4f8aab59adce45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07f76517a0f24e05ab7739d51c77d55d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74b4820a9b3346dab8331d1993436b04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9620d6ea3c6496582a16ba34cbbf537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/230 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a26be583638d49ee8384f4d251d5b84d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b8304becdcb4d6ba78b273cddb0b3dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc57c67d645e4839b2a82cfc2721c08f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.12.4 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"srivatsaHFhub/llama3.1_fineTomeAlpaca_modified\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=False,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    use_gradient_checkpointing=True,\n",
    "    use_rslora=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpo_dataset = load_dataset(\"mlabonne/llmtwin-dpo\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>rejected</th>\n",
       "      <th>chosen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>How can the comparability of node values be en...</td>\n",
       "      <td>The comparability of node values can be ensure...</td>\n",
       "      <td>To ensure a similar range of values for all no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>Why is cost consideration important in choosin...</td>\n",
       "      <td>Budget constraints significantly influence the...</td>\n",
       "      <td>Budget constraints will influence your choice ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1109</th>\n",
       "      <td>How are settings defined for configuring infer...</td>\n",
       "      <td>Settings for configuring inference pipeline co...</td>\n",
       "      <td>We used pydantic_settings and inherited its Ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>What must be done for every type of operation ...</td>\n",
       "      <td>For every type of operation needing output in ...</td>\n",
       "      <td>Next, for every type of operation we need outp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>What should I do to merge the base Code Llama ...</td>\n",
       "      <td>To merge the base Code Llama model with the QL...</td>\n",
       "      <td>However, you can also merge the base Code Llam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>What should be considered when using LLM algor...</td>\n",
       "      <td>When using LLM algorithms in production, it is...</td>\n",
       "      <td>Using LLM algorithms in production is not all ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>How does one load a custom model using HookedT...</td>\n",
       "      <td>To load a custom model using HookedTransformer...</td>\n",
       "      <td>Unfortunately, you cant directly load a custom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>What will be covered in Lesson 6?</td>\n",
       "      <td>In Lesson 6, the focus will shift to the train...</td>\n",
       "      <td>In Lesson 6, we will move to the training pipe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1424</th>\n",
       "      <td>What strategy is suggested when starting a pro...</td>\n",
       "      <td>A good strategy is to write a wrapper over an ...</td>\n",
       "      <td>When you start a project, a good strategy is t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>How does Lesson 4 relate to the role of featur...</td>\n",
       "      <td>Lesson 4 demonstrates how to clean, chunk, emb...</td>\n",
       "      <td>In Lesson 4 , we showed you how to clean, chun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 prompt  \\\n",
       "613   How can the comparability of node values be en...   \n",
       "394   Why is cost consideration important in choosin...   \n",
       "1109  How are settings defined for configuring infer...   \n",
       "235   What must be done for every type of operation ...   \n",
       "588   What should I do to merge the base Code Llama ...   \n",
       "381   What should be considered when using LLM algor...   \n",
       "452   How does one load a custom model using HookedT...   \n",
       "1089                  What will be covered in Lesson 6?   \n",
       "1424  What strategy is suggested when starting a pro...   \n",
       "283   How does Lesson 4 relate to the role of featur...   \n",
       "\n",
       "                                               rejected  \\\n",
       "613   The comparability of node values can be ensure...   \n",
       "394   Budget constraints significantly influence the...   \n",
       "1109  Settings for configuring inference pipeline co...   \n",
       "235   For every type of operation needing output in ...   \n",
       "588   To merge the base Code Llama model with the QL...   \n",
       "381   When using LLM algorithms in production, it is...   \n",
       "452   To load a custom model using HookedTransformer...   \n",
       "1089  In Lesson 6, the focus will shift to the train...   \n",
       "1424  A good strategy is to write a wrapper over an ...   \n",
       "283   Lesson 4 demonstrates how to clean, chunk, emb...   \n",
       "\n",
       "                                                 chosen  \n",
       "613   To ensure a similar range of values for all no...  \n",
       "394   Budget constraints will influence your choice ...  \n",
       "1109  We used pydantic_settings and inherited its Ba...  \n",
       "235   Next, for every type of operation we need outp...  \n",
       "588   However, you can also merge the base Code Llam...  \n",
       "381   Using LLM algorithms in production is not all ...  \n",
       "452   Unfortunately, you cant directly load a custom...  \n",
       "1089  In Lesson 6, we will move to the training pipe...  \n",
       "1424  When you start a project, a good strategy is t...  \n",
       "283   In Lesson 4 , we showed you how to clean, chun...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpo_dataset.to_pandas().sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>rejected</th>\n",
       "      <th>chosen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'content': 'You are a helpful assistant. Bel...</td>\n",
       "      <td>[{'content': 'The approach involves sticking t...</td>\n",
       "      <td>[{'content': 'For the other serverless tools Q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'content': 'You are a helpful assistant. Bel...</td>\n",
       "      <td>[{'content': 'The course is created by Paul Iu...</td>\n",
       "      <td>[{'content': 'The course is created under the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'content': 'You are a helpful assistant. Bel...</td>\n",
       "      <td>[{'content': 'In Lesson 2, we will learn how t...</td>\n",
       "      <td>[{'content': 'In Lesson 2, we will learn how t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'content': 'You are a helpful assistant. Bel...</td>\n",
       "      <td>[{'content': 'The primary purpose of the CDC c...</td>\n",
       "      <td>[{'content': 'CDC's primary purpose is to iden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'content': 'You are a helpful assistant. Bel...</td>\n",
       "      <td>[{'content': 'The LLM Twin course focuses on t...</td>\n",
       "      <td>[{'content': 'By finishing the LLM Twin Buildi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[{'content': 'You are a helpful assistant. Bel...</td>\n",
       "      <td>[{'content': 'An LLM Twin's architecture consi...</td>\n",
       "      <td>[{'content': 'The architecture of the LLM twin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[{'content': 'You are a helpful assistant. Bel...</td>\n",
       "      <td>[{'content': 'Participants will learn MLOps be...</td>\n",
       "      <td>[{'content': 'You will also learn to leverage ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[{'content': 'You are a helpful assistant. Bel...</td>\n",
       "      <td>[{'content': 'A data pipeline acts as the engi...</td>\n",
       "      <td>[{'content': 'This automated system acts as th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[{'content': 'You are a helpful assistant. Bel...</td>\n",
       "      <td>[{'content': 'Key reasons include efficiency a...</td>\n",
       "      <td>[{'content': 'Here are some key reasons Effici...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[{'content': 'You are a helpful assistant. Bel...</td>\n",
       "      <td>[{'content': 'A data pipeline standardizes dat...</td>\n",
       "      <td>[{'content': 'Pipelines standardize data handl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  [{'content': 'You are a helpful assistant. Bel...   \n",
       "1  [{'content': 'You are a helpful assistant. Bel...   \n",
       "2  [{'content': 'You are a helpful assistant. Bel...   \n",
       "3  [{'content': 'You are a helpful assistant. Bel...   \n",
       "4  [{'content': 'You are a helpful assistant. Bel...   \n",
       "5  [{'content': 'You are a helpful assistant. Bel...   \n",
       "6  [{'content': 'You are a helpful assistant. Bel...   \n",
       "7  [{'content': 'You are a helpful assistant. Bel...   \n",
       "8  [{'content': 'You are a helpful assistant. Bel...   \n",
       "9  [{'content': 'You are a helpful assistant. Bel...   \n",
       "\n",
       "                                            rejected  \\\n",
       "0  [{'content': 'The approach involves sticking t...   \n",
       "1  [{'content': 'The course is created by Paul Iu...   \n",
       "2  [{'content': 'In Lesson 2, we will learn how t...   \n",
       "3  [{'content': 'The primary purpose of the CDC c...   \n",
       "4  [{'content': 'The LLM Twin course focuses on t...   \n",
       "5  [{'content': 'An LLM Twin's architecture consi...   \n",
       "6  [{'content': 'Participants will learn MLOps be...   \n",
       "7  [{'content': 'A data pipeline acts as the engi...   \n",
       "8  [{'content': 'Key reasons include efficiency a...   \n",
       "9  [{'content': 'A data pipeline standardizes dat...   \n",
       "\n",
       "                                              chosen  \n",
       "0  [{'content': 'For the other serverless tools Q...  \n",
       "1  [{'content': 'The course is created under the ...  \n",
       "2  [{'content': 'In Lesson 2, we will learn how t...  \n",
       "3  [{'content': 'CDC's primary purpose is to iden...  \n",
       "4  [{'content': 'By finishing the LLM Twin Buildi...  \n",
       "5  [{'content': 'The architecture of the LLM twin...  \n",
       "6  [{'content': 'You will also learn to leverage ...  \n",
       "7  [{'content': 'This automated system acts as th...  \n",
       "8  [{'content': 'Here are some key reasons Effici...  \n",
       "9  [{'content': 'Pipelines standardize data handl...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def apply_basic_preference_template(example):\n",
    "    prompt = [\n",
    "        {\"role\": \"system\", \"content\":\"You are a helpful assistant. Below is an instruction that describes a task. Write a response that appropriately completes the request.\"},\n",
    "        {\"role\": \"user\", \"content\": example[\"prompt\"]},\n",
    "    ]\n",
    "    chosen = [{\"role\": \"assistant\", \"content\": example[\"chosen\"]}]\n",
    "    rejected = [{\"role\": \"assistant\", \"content\": example[\"rejected\"]}]\n",
    "    \n",
    "    preference_example = {\"prompt\": prompt,\n",
    "                      \"chosen\": chosen,\n",
    "                      \"rejected\": rejected}\n",
    "    \n",
    "    return preference_example\n",
    "\n",
    "\n",
    "dpo_dataset = dpo_dataset.map(apply_basic_preference_template, remove_columns=[\"prompt\", \"rejected\", \"chosen\"])\n",
    "\n",
    "dpo_dataset.to_pandas().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee35250183e8426491331e079972f7dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1545 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>rejected</th>\n",
       "      <th>chosen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;assistant&lt;...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;assistant&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;assistant&lt;...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;assistant&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1479</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;assistant&lt;...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;assistant&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1320</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;assistant&lt;...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;assistant&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;assistant&lt;...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;assistant&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;assistant&lt;...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;assistant&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;assistant&lt;...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;assistant&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;assistant&lt;...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;assistant&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;assistant&lt;...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;assistant&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|en...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;assistant&lt;...</td>\n",
       "      <td>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;assistant&lt;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 prompt  \\\n",
       "225   <|begin_of_text|><|start_header_id|>system<|en...   \n",
       "747   <|begin_of_text|><|start_header_id|>system<|en...   \n",
       "1479  <|begin_of_text|><|start_header_id|>system<|en...   \n",
       "1320  <|begin_of_text|><|start_header_id|>system<|en...   \n",
       "1209  <|begin_of_text|><|start_header_id|>system<|en...   \n",
       "993   <|begin_of_text|><|start_header_id|>system<|en...   \n",
       "343   <|begin_of_text|><|start_header_id|>system<|en...   \n",
       "1487  <|begin_of_text|><|start_header_id|>system<|en...   \n",
       "1211  <|begin_of_text|><|start_header_id|>system<|en...   \n",
       "362   <|begin_of_text|><|start_header_id|>system<|en...   \n",
       "\n",
       "                                               rejected  \\\n",
       "225   <|begin_of_text|><|start_header_id|>assistant<...   \n",
       "747   <|begin_of_text|><|start_header_id|>assistant<...   \n",
       "1479  <|begin_of_text|><|start_header_id|>assistant<...   \n",
       "1320  <|begin_of_text|><|start_header_id|>assistant<...   \n",
       "1209  <|begin_of_text|><|start_header_id|>assistant<...   \n",
       "993   <|begin_of_text|><|start_header_id|>assistant<...   \n",
       "343   <|begin_of_text|><|start_header_id|>assistant<...   \n",
       "1487  <|begin_of_text|><|start_header_id|>assistant<...   \n",
       "1211  <|begin_of_text|><|start_header_id|>assistant<...   \n",
       "362   <|begin_of_text|><|start_header_id|>assistant<...   \n",
       "\n",
       "                                                 chosen  \n",
       "225   <|begin_of_text|><|start_header_id|>assistant<...  \n",
       "747   <|begin_of_text|><|start_header_id|>assistant<...  \n",
       "1479  <|begin_of_text|><|start_header_id|>assistant<...  \n",
       "1320  <|begin_of_text|><|start_header_id|>assistant<...  \n",
       "1209  <|begin_of_text|><|start_header_id|>assistant<...  \n",
       "993   <|begin_of_text|><|start_header_id|>assistant<...  \n",
       "343   <|begin_of_text|><|start_header_id|>assistant<...  \n",
       "1487  <|begin_of_text|><|start_header_id|>assistant<...  \n",
       "1211  <|begin_of_text|><|start_header_id|>assistant<...  \n",
       "362   <|begin_of_text|><|start_header_id|>assistant<...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"llama-3\",\n",
    "    map_eos_token=True\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    prompt = [tokenizer.apply_chat_template(txt, tokenize=False, add_generation_prompt=False) for txt in examples[\"prompt\"]]\n",
    "    chosen = [tokenizer.apply_chat_template(txt, tokenize=False, add_generation_prompt=False) for txt in examples[\"chosen\"]]\n",
    "    rejected = [tokenizer.apply_chat_template(txt, tokenize=False, add_generation_prompt=False) for txt in examples[\"rejected\"]]\n",
    "    return {\"prompt\": prompt, \"chosen\": chosen, \"rejected\": rejected}\n",
    "\n",
    "dpo_dataset_formatted = dpo_dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "dpo_dataset_formatted.to_pandas().sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'rejected', 'chosen'],\n",
       "        num_rows: 1467\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'rejected', 'chosen'],\n",
       "        num_rows: 78\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpo_dataset = dpo_dataset_formatted.train_test_split(test_size=0.05)\n",
    "dpo_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb4fd1eb0c4434695fa8c86aa439ffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt from train dataset:   0%|          | 0/1467 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd6e003b7a694b3986147946adaf7d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/1467 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c58406c1ee4b9488105a8961b9ce42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt from eval dataset:   0%|          | 0/78 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8416d0136c6d4205adc89158c1583934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset:   0%|          | 0/78 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce59e7af96e54f9aaac37134a46af3f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/1467 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d45a21aeb674d3fbca3e1c8dd21a2a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/78 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dpo_trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=None,\n",
    "    tokenizer=tokenizer,\n",
    "    beta=0.5,\n",
    "    train_dataset=dpo_dataset[\"train\"],\n",
    "    eval_dataset=dpo_dataset[\"test\"],\n",
    "    max_length = max_seq_length//2,\n",
    "    max_prompt_length = max_seq_length//2,\n",
    "    args = DPOConfig(\n",
    "        learning_rate=2e-6,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs=1,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=10,\n",
    "        output_dir=\"output\",\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=0.2,\n",
    "        logging_steps=1,\n",
    "        report_to=\"comet_ml\",\n",
    "        seed=432\n",
    "    )\n",
    ")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 1,467 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 16 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 64 | Total steps = 23\n",
      " \"-____-\"     Number of trainable parameters = 83,886,080\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m An experiment with the same configuration options is already running and will be reused.\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23' max='23' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [23/23 04:20, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>rewards / chosen</th>\n",
       "      <th>rewards / rejected</th>\n",
       "      <th>rewards / accuracies</th>\n",
       "      <th>rewards / margins</th>\n",
       "      <th>logps / rejected</th>\n",
       "      <th>logps / chosen</th>\n",
       "      <th>logits / rejected</th>\n",
       "      <th>logits / chosen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.698000</td>\n",
       "      <td>0.708744</td>\n",
       "      <td>-0.022379</td>\n",
       "      <td>-0.004036</td>\n",
       "      <td>0.453571</td>\n",
       "      <td>-0.018343</td>\n",
       "      <td>-87.792160</td>\n",
       "      <td>-127.291100</td>\n",
       "      <td>1.959054</td>\n",
       "      <td>1.928802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.691000</td>\n",
       "      <td>0.688704</td>\n",
       "      <td>0.013514</td>\n",
       "      <td>-0.006023</td>\n",
       "      <td>0.519643</td>\n",
       "      <td>0.019537</td>\n",
       "      <td>-87.796143</td>\n",
       "      <td>-127.219315</td>\n",
       "      <td>1.959500</td>\n",
       "      <td>1.929515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.658100</td>\n",
       "      <td>0.667147</td>\n",
       "      <td>0.044929</td>\n",
       "      <td>-0.018907</td>\n",
       "      <td>0.619643</td>\n",
       "      <td>0.063835</td>\n",
       "      <td>-87.821907</td>\n",
       "      <td>-127.156494</td>\n",
       "      <td>1.959986</td>\n",
       "      <td>1.929285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.646300</td>\n",
       "      <td>0.666166</td>\n",
       "      <td>0.064059</td>\n",
       "      <td>-0.006534</td>\n",
       "      <td>0.644643</td>\n",
       "      <td>0.070593</td>\n",
       "      <td>-87.797165</td>\n",
       "      <td>-127.118240</td>\n",
       "      <td>1.960242</td>\n",
       "      <td>1.929052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : secure_possum_7388\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/devsrivatsa/llm-engineers-handbook-dpo/0dd5abe8fb1948c98d3ee01b252d2fdb\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval/logits/chosen [4]         : (1.928802251815796, 1.9295152425765991)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval/logits/rejected [4]       : (1.9590539932250977, 1.960241675376892)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval/logps/chosen [4]          : (-127.29109954833984, -127.11824035644531)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval/logps/rejected [4]        : (-87.82190704345703, -87.79216003417969)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval/loss [4]                  : (0.6661661863327026, 0.7087441086769104)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval/rewards/accuracies [4]    : (0.4535714089870453, 0.6446428894996643)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval/rewards/chosen [4]        : (-0.022378820925951004, 0.06405866891145706)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval/rewards/margins [4]       : (-0.018342535942792892, 0.0705927237868309)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval/rewards/rejected [4]      : (-0.018906742334365845, -0.00403628358617425)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval/runtime [4]               : (5.9703, 7.9879)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval/samples_per_second [4]    : (9.765, 13.065)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval/steps_per_second [4]      : (0.626, 0.837)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss [9]                       : (0.6067025661468506, 0.7195209264755249)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/epoch [28]               : (0.043478260869565216, 1.0)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/grad_norm [23]           : (28.496803283691406, 36.99300003051758)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/learning_rate [23]       : (0.0, 2e-06)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/logits/chosen [23]       : (1.8059124946594238, 2.1305599212646484)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/logits/rejected [23]     : (1.850329875946045, 2.102780818939209)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/logps/chosen [23]        : (-133.08432006835938, -111.087890625)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/logps/rejected [23]      : (-92.58190155029297, -83.12079620361328)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/loss [23]                : (0.6414, 0.7191)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/rewards/accuracies [23]  : (0.0, 0.75)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/rewards/chosen [23]      : (-0.04275786876678467, 0.10295629501342773)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/rewards/margins [23]     : (-0.04466754198074341, 0.11527425050735474)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/rewards/rejected [23]    : (-0.02962327003479004, 0.022071808576583862)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/total_flos               : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/train_loss               : 0.6772100147993668\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/train_runtime            : 272.8689\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/train_samples_per_second : 5.376\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train/train_steps_per_second   : 0.084\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     hasNestedParams : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|accelerator_config|dispatch_batches             : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|accelerator_config|even_batches                 : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|accelerator_config|gradient_accumulation_kwargs : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|accelerator_config|non_blocking                 : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|accelerator_config|split_batches                : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|accelerator_config|use_seedable_sampler         : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|adafactor                                       : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|adam_beta1                                      : 0.9\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|adam_beta2                                      : 0.999\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|adam_epsilon                                    : 1e-08\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|auto_find_batch_size                            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|average_tokens_across_devices                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|batch_eval_metrics                              : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|beta                                            : 0.5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|bf16                                            : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|bf16_full_eval                                  : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|data_seed                                       : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|dataloader_drop_last                            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|dataloader_num_workers                          : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|dataloader_persistent_workers                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|dataloader_pin_memory                           : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|dataloader_prefetch_factor                      : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|dataset_num_proc                                : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|ddp_backend                                     : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|ddp_broadcast_buffers                           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|ddp_bucket_cap_mb                               : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|ddp_find_unused_parameters                      : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|ddp_timeout                                     : 1800\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|debug                                           : []\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|deepspeed                                       : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|disable_dropout                                 : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|disable_tqdm                                    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|dispatch_batches                                : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|do_eval                                         : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|do_predict                                      : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|do_train                                        : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|eval_accumulation_steps                         : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|eval_delay                                      : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|eval_do_concat_batches                          : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|eval_on_start                                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|eval_steps                                      : 0.2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|eval_strategy                                   : steps\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|eval_use_gather_object                          : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|evaluation_strategy                             : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|f_alpha_divergence_coef                         : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|f_divergence_type                               : reverse_kl\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|force_use_ref_model                             : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|fp16                                            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|fp16_backend                                    : auto\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|fp16_full_eval                                  : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|fp16_opt_level                                  : O1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|fsdp                                            : []\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|fsdp_config|min_num_params                      : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|fsdp_config|xla                                 : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|fsdp_config|xla_fsdp_grad_ckpt                  : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|fsdp_config|xla_fsdp_v2                         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|fsdp_min_num_params                             : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|fsdp_transformer_layer_cls_to_wrap              : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|full_determinism                                : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|generate_during_eval                            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|gradient_accumulation_steps                     : 4\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|gradient_checkpointing                          : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|gradient_checkpointing_kwargs                   : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|greater_is_better                               : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|group_by_length                                 : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|half_precision_backend                          : auto\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|hub_always_push                                 : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|hub_model_id                                    : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|hub_private_repo                                : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|hub_strategy                                    : every_save\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|hub_token                                       : <HUB_TOKEN>\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|ignore_data_skip                                : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|include_for_metrics                             : []\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|include_inputs_for_metrics                      : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|include_num_input_tokens_seen                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|include_tokens_per_second                       : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|is_encoder_decoder                              : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|jit_mode_eval                                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|label_names                                     : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|label_pad_token_id                              : -100\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|label_smoothing                                 : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|label_smoothing_factor                          : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|learning_rate                                   : 2e-06\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|length_column_name                              : length\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|load_best_model_at_end                          : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|local_rank                                      : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|log_level                                       : passive\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|log_level_replica                               : warning\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|log_on_each_node                                : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|logging_dir                                     : output/runs/Dec10_02-54-39_b1c5d83db634\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|logging_first_step                              : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|logging_nan_inf_filter                          : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|logging_steps                                   : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|logging_strategy                                : steps\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|loss_type                                       : sigmoid\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|lr_scheduler_kwargs                             : {}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|lr_scheduler_type                               : linear\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|max_completion_length                           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|max_grad_norm                                   : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|max_length                                      : 1024\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|max_prompt_length                               : 1024\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|max_steps                                       : -1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|max_target_length                               : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|metric_for_best_model                           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|model_adapter_name                              : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|model_init_kwargs                               : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|mp_parameters                                   : \n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|neftune_noise_alpha                             : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|no_cuda                                         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|num_train_epochs                                : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|optim                                           : adamw_8bit\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|optim_args                                      : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|optim_target_modules                            : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|output_dir                                      : output\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|overwrite_output_dir                            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|padding_value                                   : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|past_index                                      : -1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|per_device_eval_batch_size                      : 16\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|per_device_train_batch_size                     : 16\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|per_gpu_eval_batch_size                         : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|per_gpu_train_batch_size                        : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|precompute_ref_log_probs                        : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|prediction_loss_only                            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|push_to_hub                                     : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|push_to_hub_model_id                            : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|push_to_hub_organization                        : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|push_to_hub_token                               : <PUSH_TO_HUB_TOKEN>\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|ray_scope                                       : last\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|ref_adapter_name                                : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|ref_model_init_kwargs                           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|ref_model_mixup_alpha                           : 0.9\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|ref_model_sync_steps                            : 64\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|reference_free                                  : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|remove_unused_columns                           : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|report_to                                       : ['comet_ml']\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|restore_callback_states_from_checkpoint         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|resume_from_checkpoint                          : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|rpo_alpha                                       : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|run_name                                        : output\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|save_on_each_node                               : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|save_only_model                                 : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|save_safetensors                                : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|save_steps                                      : 500\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|save_strategy                                   : steps\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|save_total_limit                                : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|seed                                            : 432\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|skip_memory_metrics                             : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|split_batches                                   : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|sync_ref_model                                  : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|tf32                                            : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|torch_compile                                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|torch_compile_backend                           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|torch_compile_mode                              : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|torch_empty_cache_steps                         : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|torchdynamo                                     : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|tpu_metrics_debug                               : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|tpu_num_cores                                   : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|truncation_mode                                 : keep_end\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|use_cpu                                         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|use_ipex                                        : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|use_legacy_prediction_loop                      : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|use_liger_kernel                                : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|use_mps_device                                  : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|use_weighting                                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|warmup_ratio                                    : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|warmup_steps                                    : 10\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args|weight_decay                                    : 0.01\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|_attn_implementation_autoset                  : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|_name_or_path                                 : srivatsaHFhub/llama3.1_fineTomeAlpaca_modified\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|add_cross_attention                           : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|architectures                                 : ['LlamaForCausalLM']\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|attention_bias                                : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|attention_dropout                             : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|bad_words_ids                                 : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|begin_suppress_tokens                         : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|bos_token_id                                  : 128000\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|chunk_size_feed_forward                       : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|cross_attention_hidden_size                   : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|decoder_start_token_id                        : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|diversity_penalty                             : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|do_sample                                     : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|early_stopping                                : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|encoder_no_repeat_ngram_size                  : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|eos_token_id                                  : 128001\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|exponential_decay_length_penalty              : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|finetuning_task                               : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|forced_bos_token_id                           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|forced_eos_token_id                           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|head_dim                                      : 128\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|hidden_act                                    : silu\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|hidden_size                                   : 4096\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|id2label|0                                    : LABEL_0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|id2label|1                                    : LABEL_1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|initializer_range                             : 0.02\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|intermediate_size                             : 14336\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|is_decoder                                    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|is_encoder_decoder                            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|label2id|LABEL_0                              : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|label2id|LABEL_1                              : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|length_penalty                                : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|max_length                                    : 20\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|max_position_embeddings                       : 131072\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|min_length                                    : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|mlp_bias                                      : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|model_type                                    : llama\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|no_repeat_ngram_size                          : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|num_attention_heads                           : 32\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|num_beam_groups                               : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|num_beams                                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|num_hidden_layers                             : 32\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|num_key_value_heads                           : 8\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|num_return_sequences                          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|output_attentions                             : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|output_hidden_states                          : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|output_scores                                 : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|pad_token_id                                  : 128004\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|prefix                                        : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|pretraining_tp                                : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|problem_type                                  : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|pruned_heads                                  : {}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|remove_invalid_values                         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|repetition_penalty                            : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|return_dict                                   : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|return_dict_in_generate                       : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|rms_norm_eps                                  : 1e-05\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|rope_scaling|factor                           : 8.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|rope_scaling|high_freq_factor                 : 4.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|rope_scaling|low_freq_factor                  : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|rope_scaling|original_max_position_embeddings : 8192\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|rope_scaling|rope_type                        : llama3\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|rope_theta                                    : 500000.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|sep_token_id                                  : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|suppress_tokens                               : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|task_specific_params                          : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|temperature                                   : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|tf_legacy_loss                                : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|tie_encoder_decoder                           : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|tie_word_embeddings                           : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|tokenizer_class                               : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|top_k                                         : 50\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|top_p                                         : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|torch_dtype                                   : bfloat16\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|torchscript                                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|transformers_version                          : 4.46.3\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|typical_p                                     : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|unsloth_version                               : 2024.12.4\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|use_bfloat16                                  : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|use_cache                                     : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config|vocab_size                                    : 128256\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     peft_config|default                                  : LoraConfig(task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='srivatsaHFhub/llama3.1_fineTomeAlpaca_modified', revision=None, inference_mode=False, r=32, target_modules={'o_proj', 'k_proj', 'v_proj', 'gate_proj', 'q_proj', 'down_proj', 'up_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, eva_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename            : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model graph         : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook            : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages         : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code         : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------Preference Training Complete--------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dpo_trainer.train()\n",
    "print(\"\\n------------------Preference Training Complete--------------------------\\n\")\n",
    "exp.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: You are pushing to hub, but you passed your HF username = srivatsaHFhub.\n",
      "We shall truncate srivatsaHFhub/llama3.1_fineTomeAlpaca_modified_aligned to llama3.1_fineTomeAlpaca_modified_aligned\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 330.56 out of 503.73 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 44.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer..."
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "177744358abd4eaa94bc7917888e8020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7ed3d1dec254e819669f252dc939caf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/618 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bf706152917428a8defe6665b12f5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "814cea82fc7348f28ed823fcab0d4afe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d02e5442784d50b309a1892b09caff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ba506b256c4b238c8f7f73c9d4ad4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99878a0634c24c398067ba62b7055c7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Saved merged model to https://huggingface.co/srivatsaHFhub/llama3.1_fineTomeAlpaca_modified_aligned\n"
     ]
    }
   ],
   "source": [
    "model.push_to_hub_merged(\"srivatsaHFhub/llama3.1_fineTomeAlpaca_modified_aligned\", tokenizer, save_method=\"merged_16bit\", token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervised fine tuning is a technique used to improve the performance of pre-trained language models by training them on a specific dataset. This process involves using a labeled dataset to guide the model in understanding the nuances of the target task, such as sentiment analysis or question answering. By providing the model with examples of how to respond to different inputs, supervised fine tuning helps to refine the model's outputs, making them more relevant and accurate. This method is particularly useful when the model needs to be tailored to a particular domain or application, as it allows for a more targeted approach to training. In summary, supervised fine tuning is a crucial step in enhancing the capabilities of language models, ensuring they can effectively handle specific tasks.<|reserved_special_token_34|><|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "generate(\"Can you tell me about supervised fine tuning ?\",  max_new_tokens=1024, temp=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct preference optimization aims to train a model based on a single reward function, which is a binary indicator that specifies whether a selected action is preferred or not. This method can be effective in scenarios where the preference is well-defined, such as in classification tasks. However, it does come with the caveat that the reward function must be correctly specified and aligned with the model's objectives. If the reward function is misaligned, the training can lead to suboptimal outcomes, highlighting the importance of accurate reward design.<|reserved_special_token_138|><|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "generate(\"How does direct preference optimization work ?\", max_new_tokens=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature is a crucial parameter in the context of LLMs, as it directly influences the randomness of the model's output. By adjusting the temperature, we can control the level of variability in the generated text. A higher temperature value, such as 0.7, leads to more diverse and creative responses, while a lower temperature, like 0.1, results in more deterministic and predictable outputs. This flexibility allows users to tailor the model's behavior to their specific needs, whether they seek imaginative solutions or precise, fact-based responses. The ability to manipulate temperature is a powerful tool for fine-tuning the model's performance in various applications.<|reserved_special_token_115|><|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "generate(\"How does temperature affect LLM output ?\", temp=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation: There is some change in the style that the model responds.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
